### 网络系统

#### 零拷贝

##### 传统I/O访问磁盘

用户进程会一直阻塞。

CPU需要在收到磁盘的I/O中断后，将数据从磁盘控制器中搬到Page Cache，再从Page Cache搬到用户内存。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png)

##### DMA工作过程

将整个I/O请求转交DMA处理，CPU不参与数据搬运的工作。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)

##### 传统socket文件传输

两行代码，四次内核态/用户态上下文切换（两次系统调用），四次数据拷贝（两次DMA，两次CPU）

```cpp
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)

优化思路：去掉用户缓冲区的两次拷贝。

##### 零拷贝

###### mmap+write

```c
buf = mmap(file, len);
write(sockfd, buf, len);
```

mmap:将内核缓冲区内容**映射**到用户缓冲区，用户缓冲区与内核不再需要数据拷贝。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)

减少一次CPU拷贝。三次拷贝+4次上下文切换

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 **CPU** 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

###### sendfile

在Linux2.1版本中定义。

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

将前文中的read()+write(socket)合并成一次系统调用。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)

一次系统调用+三次拷贝。

###### 零拷贝

对于支持SG-DMA（*The Scatter-Gather Direct Memory Access*）的网卡，可以直接将内核缓冲区数据通过DMA拷贝到网卡上。全程不需要CPU搬数据。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)



NGINX支持零拷贝，可以把sendfile on，就能用了。

##### PageCache

零拷贝中的内核缓冲区实际上是Page Cache，

1. **PageCache 来缓存最近被访问的磁盘数据**。
2. **PageCache 使用了「预读功能」**

（3. 会把多个I/O请求合并成一个chunk再读）

##### 大文件传输

**尽量不要用零拷贝**

原因：Page Cache的预取会使DMA 多做一次数据拷贝，且其他「热点」的小文件可能就无法充分使用到 PageCache



###### 异步I/O+直接I/O

一般对于磁盘，异步I/O只支持直接I/O

异步：读取后直接返回，等待内核读完后通知自己再处理I/O

直接：不用PageCache，**直接从磁盘缓冲区读到用户缓冲区**。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png)

###### nginx配置

大文件用异步+直接，小文件用零拷贝

```reStructuredText
location /video/ { 
    sendfile on; 
    aio on; 
    directio 1024m; 
}
```

#### I/O多路复用

##### socket模型

服务端bind:绑定ip（代表指定的网卡）和端口（表示要传给当前应用）

listen:监听该ip：端口号

accept:**阻塞**等待连接的到来，并处理。



客户端：connect，指定服务端ip和端口号。开始三次握手。

未完成三次握手的队列：半连接队列。已完成的：全连接队列。

accept从全连接队列里取一个，作为后续数据传输的socket。



以下是一对一socket示例图。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/tcp_socket.png)

socket实际上是**文件**，服务端中需要为每个连接上的socket对应一个文件描述符

##### 多进程模型

父进程fork一份，将自己的文件描述符，地址空间等都给子进程一份。

父进程只关心监听，子进程只关心已连接的socket。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%BF%9B%E7%A8%8B.png)

父进程需要wait()/waitpid()，否则如果子进程先结束，会产生僵尸进程（子进程的pid等不会被正确释放）。

但进程切换开销很大。

##### 多线程模型

创建线程池，将已连接的socket放到一个队列里，线程每次取一个来处理。

##### I/O多路复用

在**一个进程处理多个文件的I/O**

本质上是分时多路复用（1ms处理一个请求，1s处理1000个请求）

select/poll/epoll ：内核提供给用户态的**多路复用系统调用**，**进程可以通过一个系统调用函数从内核中获取多个I/O事件**。

思路：先把所有连接从用户态传给内核，由内核返回产生了事件的连接，然后再在用户态处理。

###### select/poll

1. 将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里
2. 内核检查：通过**遍历**文件描述符集合的方式，当检查到有**事件**产生后，将此 Socket **标记为可读或可写**
3. 再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。

两次遍历+两次拷贝

时间复杂度：O（N）

poll：动态数组，select是静态数组，受数组大小限制



###### epoll

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...)

int epfd = epoll_create(...); //创建epoll对象
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1) {
    int n = epoll_wait(...); 
    for(接收到数据的socket){
        //处理
    }
}
```

1. 在内核**维护红黑树来跟踪fd**（红黑树的增删改复杂度O(logn)）
2. **事件驱动**机制：当某个socket有事件发生时，通过**回调函数**，内核将其加到一个**就绪列表**中

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png)

边缘触发：只在第一次遇到事件时通知

水平触发：未处理的事件都会被通知

一般用边缘触发+非阻塞I/O（可以执行尽可能多的）（防止无法执行的I/O阻塞住进程）。



非阻塞I/O和异步I/O的区别：非阻塞：不需要进程/线程**干等着**，直接返回

异步：操作系统自己执行并把数据放到用户态，然后**主动通知**用户已就绪